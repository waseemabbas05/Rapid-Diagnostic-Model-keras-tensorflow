{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dl framwork - tensorflow, keras a backend \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization \n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import glob\n",
    "#from helpers import load_folder\n",
    "#from helpers import gen_data\n",
    "\n",
    "#data_folder = '/home/wabbas/shared/datasets/covid19/chest_xray'\n",
    "# data folder should contain 3 more folders named 'train', 'val' and 'test'\n",
    "# each of these sub-folders should contain two folders, one containing images of control group named \"NORMAL\" \n",
    "# the other containing x-ray images of pneumonia patients, named \"PNEUMONIA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 3}, log_device_placement=True ) \n",
    "sess = tf.compat.v1.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_concat(x1,x2):\n",
    "    with tf.name_scope(\"crop_and_concat\"):\n",
    "        return tf.concat([x1, x2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = os.path.join(data_folder,'train')\n",
    "test_folder = os.path.join(data_folder,'test')\n",
    "val_folder = os.path.join(data_folder,'val')\n",
    "train_normal_path = os.path.join(train_folder,'NORMAL')\n",
    "train_pneumonia_path = os.path.join(train_folder,'PNEUMONIA')\n",
    "\n",
    "val_normal_path = os.path.join(val_folder,'NORMAL')\n",
    "val_pneumonia_path = os.path.join(val_folder,'PNEUMONIA')\n",
    "\n",
    "test_normal_path = os.path.join(test_folder,'NORMAL')\n",
    "test_pneumonia_path = os.path.join(test_folder,'PNEUMONIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/wabbas/shared/datasets/covid19/chest_xray/\"\n",
    "\n",
    "# data folder should contain 3 more folders named 'train', 'val' and 'test'\n",
    "# each of these sub-folders should contain two folders, one containing images of control group named \"NORMAL\" \n",
    "# the other containing x-ray images of pneumonia patients, named \"PNEUMONIA\"\n",
    "\n",
    "    \n",
    "for _set in ['train', 'test', 'val']:\n",
    "    nrml = len(os.listdir(input_path + _set + '/NORMAL'))\n",
    "    pnm = len(os.listdir(input_path + _set + '/PNEUMONIA'))\n",
    "    print('{}, Normal images: {}, Pneumonia images: {}'.format(_set, nrml, pnm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(img_dims, batch_size):\n",
    "    # Data generation objects - thorugh rescalling, veticle flip, zoom range\n",
    "    train_datagen = ImageDataGenerator(\n",
    "                        rescale = 1./255,\n",
    "                      # featurewise_center=True,\n",
    "                      # featurewise_std_normalization=True,\n",
    "                        zoom_range = 0.5,\n",
    "                        vertical_flip = True,\n",
    "                        horizontal_flip=True,\n",
    "                        featurewise_center=True,\n",
    "                        featurewise_std_normalization=True,\n",
    "                        rotation_range=30,\n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "                          zoom_range = 0.5,\n",
    "                        vertical_flip = True,\n",
    "                        horizontal_flip=True,\n",
    "                        featurewise_center=True,\n",
    "                        featurewise_std_normalization=True,\n",
    "                        rotation_range=30,\n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2,\n",
    "                        rescale=1./255)\n",
    "    \n",
    "    # This is fed to the network in the specified batch sizes and image dimensions\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "                                directory = train_folder, \n",
    "                                target_size = (img_dims, img_dims), \n",
    "                                batch_size = batch_size, \n",
    "                                class_mode = 'binary', \n",
    "                                shuffle=True)\n",
    "\n",
    "    test_gen = test_datagen.flow_from_directory(\n",
    "                                directory=test_folder, \n",
    "                                target_size=(img_dims, img_dims), \n",
    "                                batch_size=batch_size, \n",
    "                                class_mode='binary', \n",
    "                                shuffle=True)\n",
    "    \n",
    "    # Making predictions off of the test set in one batch size\n",
    "    # This is useful to be able to get the confusion matrix\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
    "        for img in (os.listdir(os.path.join(test_folder + cond))):\n",
    "            img = plt.imread(os.path.join(test_folder + cond + img))\n",
    "            img = cv2.resize(img, (img_dims, img_dims))\n",
    "            img = np.dstack([img, img, img])\n",
    "            img = img.astype('float32') / 255\n",
    "            if cond=='/NORMAL/':\n",
    "                label = 0\n",
    "            elif cond=='/PNEUMONIA/':\n",
    "                label = 1\n",
    "            test_data.append(img)\n",
    "            test_labels.append(label)\n",
    "        \n",
    "    test_data = np.array(test_data)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_gen, test_gen, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dims = 256\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Getting the data\n",
    "train_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs,num_filters,bn):\n",
    "    x1 = SeparableConv2D(filters=num_filters, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "    x1 = SeparableConv2D(filters=num_filters, kernel_size=(5, 5), activation='relu', padding='same')(x1)\n",
    "    x1 = SeparableConv2D(filters=num_filters, kernel_size=(7, 7), activation='relu', padding='same')(x1)\n",
    "\n",
    "    x2 = SeparableConv2D(filters=num_filters, kernel_size=(3, 3), activation='relu', padding='same')(x1)\n",
    "    x2 = SeparableConv2D(filters=4, kernel_size=(5, 5), activation='relu', padding='same')(x2)\n",
    "\n",
    "    x3 = SeparableConv2D(filters=num_filters, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "\n",
    "    x = crop_and_concat(x1,x2)\n",
    "    x = crop_and_concat(x,x3)\n",
    "    x = MaxPool2D(pool_size=(2,2))(x)\n",
    "    if bn:\n",
    "        x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, trainable=True) (x)\n",
    "    #x = Activation(tf.nn.sigmoid)(x)\n",
    "    return x\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "inputs = Input(shape=(img_dims, img_dims, 3))\n",
    "\n",
    "x = conv_block(inputs,16,0)\n",
    "x = conv_block(x,32,1)\n",
    "x = conv_block(x,64,1)\n",
    "x = conv_block(x,32,1)\n",
    "x = conv_block(x,16,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FC layer\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "x = Dense(units=32, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "\n",
    "\n",
    "# Output layer\n",
    "output = Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "# Creating model and compiling\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "adamc = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer= adamc, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=2, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
    "           epochs=epochs, \n",
    "           validation_data=test_gen, \n",
    "           validation_steps=test_gen.samples // batch_size,\n",
    "           callbacks=[checkpoint, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "preds = model.predict(test_data)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, np.round(preds))*100\n",
    "conf_mat = confusion_matrix(test_labels, np.round(preds))\n",
    "true_negative, false_postive, false_negative, true_posiitve = conf_mat.ravel()\n",
    "\n",
    "plot_confusion_matrix(conf_mat,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('xray-pneumona-skip-convolution.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
